{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing that HuggingFace Installed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Word Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline, set_seed\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Procedure: \n",
    "1. Load tokenizer (to break up text into pieces)\n",
    "2. Load model \n",
    "3. Encode (transform) your input into vector using tokenizer \n",
    "4. Pass encoded vector to model \n",
    "5. Use search to predict next best words \n",
    "6. Return those words \n",
    "\n",
    "For more details see: https://huggingface.co/blog/how-to-generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'max_length': 10, \n",
    "          'do_sample': True, \n",
    "          'top_k': 10, \n",
    "          'no_repeat_ngram_size': 2,  \n",
    "          'labels': inputs[\"input_ids\"]}\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(prefix, model, tokenizer, params, num_options=5): \n",
    "    inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
    "    input_len = len(inputs['input_ids'][0])\n",
    "    top_k_multi_output = model.generate(**inputs, **params, num_return_sequences=5 )\n",
    "    return top_k_multi_output[:, input_len:input_len+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"I dont feel so\"\n",
    "print(\"INPUT:{}\".format(prefix))\n",
    "b = time.time() \n",
    "output = next_word(prefix, model, tokenizer, params)\n",
    "print(\"OUTPUT:{}\".format([tokenizer.decode(i) for i in output.unique()]))\n",
    "a = time.time()\n",
    "print(\"PROCESSING TOOK:{}\".format(a-b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"I want\"\n",
    "print(\"INPUT:{}\".format(prefix))\n",
    "b = time.time() \n",
    "output = next_word(prefix, model, tokenizer, params)\n",
    "print(\"OUTPUT:{}\".format([tokenizer.decode(i) for i in output.unique()]))\n",
    "a = time.time()\n",
    "print(\"PROCESSING TOOK:{}\".format(a-b))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
